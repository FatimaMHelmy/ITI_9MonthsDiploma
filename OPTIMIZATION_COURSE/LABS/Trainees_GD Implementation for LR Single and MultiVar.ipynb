{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Work 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this practical work, the trainees will develop a Python program that is able to implement the gradient descent in order to achieve the linear regression (Single and Multivariables) of a set of datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import numpy, matplotlib.pyplot and make it inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read RegData csv file into numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('RegData.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define variables X and y. \n",
    "#### Assign first column data to X and second column to y\n",
    "<b>Note:</b> X is the independent variable (input to LR model) and y is the dependent variable (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the original data (scatter plot of X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Full Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Initialize parameters (theta_0 & theta_1) with random value or simply zero. Also choose the Learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 , theta_1, lr = 0,0,0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Use (theta_0 & theta_1) to predict the output h(x)= theta_0 + theta_1 * x.\n",
    "#### Note: you will need to iterate through all data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function of the model of LR\n",
    "h_x = theta_0 + X*theta_1\n",
    "h_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Calculate the MSE Cost function ùë±(theta_0,theta_1 ).\n",
    "<b>Note:</b> You better use either dot product or norm square of the error vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error = predicted -actual\n",
    "error = h_x - y\n",
    "# calculate cost function usin mean square error using dot product(make the summition of erroe**2)\n",
    "MSA = error @ error / (2 * len(X))\n",
    "error,MSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claculate cost function using norm2 as MSA = norm_2**2 / 2m\n",
    "MSA_NORM=(LA.norm(error,2))**2// (2 * len(X))\n",
    "MSA_NORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Calculate the gradient components for theta_0 and theta_1.\n",
    "<b>Note:</b> You can use the error vector calculated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THETA_NEW = THETA_OLD - LEARNING_RATE * DERIVATIVE COST_FUN W.R.T OLD THETA \n",
    "d_theta_0 = sum(error) # dj/d theta_0 = segma(error) /m \n",
    "d_theta_1 = error@X    #dj/d theta_1 = segma(error)*X /m\n",
    "m = len(X)\n",
    "d_theta_0=d_theta_0/m\n",
    "d_theta_1=d_theta_1/m\n",
    "d_theta_0,d_theta_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Update the parameters (simultaneously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0_new = theta_0 - lr*d_theta_0\n",
    "theta_1_new = theta_1 - lr*d_theta_1\n",
    "theta_0_new,theta_1_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6: Repeat from 2 to 5 until converge to the minimum or achieve maximum iterations.\n",
    "#### The objective from this step is to combine all the previous steps and iterate untill you either achieve the maximum number of iterations or reach the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate 1000 times to get minimum loss fun \n",
    "theta_0 =0\n",
    "theta_1 =0\n",
    "m = len(X)\n",
    "#theta_0_new = theta_0\n",
    "#theta_1_new = theta_1\n",
    "for i in range (1000):\n",
    "    # general function of the model of LR\n",
    "    h_x = theta_0 + X*theta_1\n",
    "    # error = predicted -actual\n",
    "    error = h_x - y\n",
    "    # calculate cost function usin mean square error using dot product(make the summition of erroe**2)\n",
    "    MSA = error @ error / (2 * m)\n",
    "    # THETA_NEW = THETA_OLD - LEARNING_RATE * DERIVATIVE COST_FUN W.R.T OLD THETA \n",
    "    d_theta_0 = (sum(error)/m) # dj/d theta_0 = segma(error) /m \n",
    "    d_theta_1 = (error@X) /m   #dj/d theta_1 = segma(error*X)\"dot product\" /m\n",
    "    # update the weights\n",
    "    theta_0 = theta_0 - lr*d_theta_0\n",
    "    theta_1 = theta_1 - lr*d_theta_1\n",
    "    # get norm of gradient to check if it close to zero stop iteration \n",
    "    # why norm beacuse here we have 2 componantes theta_0 and theta _1\n",
    "    gradient_vector = np.array([d_theta_0,d_theta_1])\n",
    "    gradient_vector_norm = LA.norm(gradient_vector,1)\n",
    "    if i<20 or i>990:\n",
    "        print(\"\\n###########iteration==\",i,\"#######################\")\n",
    "        print(\"j==\", MSA)\n",
    "        print(\"gradient_vector\", gradient_vector)\n",
    "        print(\"gradient_vector_norm\",gradient_vector_norm)\n",
    "    if int(gradient_vector_norm) <= 0.001 :\n",
    "        print (\"#############the gradient is close to zero############\")\n",
    "        print(\"iteration == \", i )\n",
    "        print(\"j==\", MSA)\n",
    "        print(\"gradient_vector\", gradient_vector)\n",
    "        print(\"gradient_vector_norm\",gradient_vector_norm)\n",
    "  \n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict y values using the LR equation\n",
    "##### h(x)= theta_0 + theta_1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find predicted values after updting the weights \n",
    "h_x =theta_0 + theta_1 *X\n",
    "print(\"h(x) = y_predict: \\n\",h_x)\n",
    "print(\"y_actual: \\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot  LR equation output (fitted line) with the original data (scatter plot of X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.plot(X,h_x,c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use R2 score to evaluate LR equation output\n",
    "https://en.wikipedia.org/wiki/Coefficient_of_determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_GD = h_x\n",
    "r2_score(y,y_pred_GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate 1000 times to get minimum loss fun \n",
    "theta_0 =0\n",
    "theta_1 =0\n",
    "lr=0.01\n",
    "m = len(X)\n",
    "cost_lst = []\n",
    "itert=0\n",
    "#theta_0_new = theta_0\n",
    "#theta_1_new = theta_1\n",
    "for i in range (1000):\n",
    "    # general function of the model of LR\n",
    "    h_x = theta_0 + X*theta_1\n",
    "    # error = predicted -actual\n",
    "    error = h_x - y\n",
    "    # calculate cost function usin mean square error using dot product(make the summition of erroe**2)\n",
    "    MSA = error @ error / (2 * m)\n",
    "    cost_lst.append(MSA)\n",
    "    # THETA_NEW = THETA_OLD - LEARNING_RATE * DERIVATIVE COST_FUN W.R.T OLD THETA \n",
    "    d_theta_0 = (sum(error)/m) # dj/d theta_0 = segma(error) /m \n",
    "    d_theta_1 = (error@X) /m   #dj/d theta_1 = segma(error)*X /m\n",
    "    # update the weights\n",
    "    theta_0 = theta_0 - lr*d_theta_0\n",
    "    theta_1 = theta_1 - lr*d_theta_1\n",
    "    # get norm of gradient to check if it close to zero stop iteration \n",
    "    # why norm beacuse here we have 2 componantes theta_0 and theta _1\n",
    "    gradient_vector = np.array([d_theta_0,d_theta_1])\n",
    "    gradient_vector_norm = LA.norm(gradient_vector,1)\n",
    "    if i<20 or i>990:\n",
    "        print(\"\\n###########iteration==\",i,\"#######################\")\n",
    "        print(\"j==\", MSA)\n",
    "        print(\"gradient_vector\", gradient_vector)\n",
    "        print(\"gradient_vector_norm\",gradient_vector_norm)\n",
    "    if (int(gradient_vector_norm) == 0)and (cost_lst[i] -cost_lst[i-1]<=0.01) :\n",
    "        print (\"#############the gradient is close to zero############\")\n",
    "        print(\"iteration == \", i )\n",
    "        itert=i\n",
    "        print(\"j==\", MSA)\n",
    "        print(\"gradient_vector\", gradient_vector)\n",
    "        print(\"gradient_vector_norm\",gradient_vector_norm)\n",
    "  \n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss vs. iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis=np.arange(0, itert+1, 1)\n",
    "plt.plot(x_axis,cost_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itert,len(cost_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read MultiVarLR csv file into numpy array.\n",
    "#### The first three columns are x1,x2, and x3.\n",
    "#### The last column is the target label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.genfromtxt('MultiVarLR.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data2[:,0]\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X2 = data2[:,1]\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = data2[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all features in one matix \n",
    "X_matrix=data2[:,0:3]\n",
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y2 = data2[:,-1]\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize your implementation to work for MultiVariables\n",
    "#### Vectorize your implementation.\n",
    "<b>Notes:</b> \n",
    "- You need to add column that contains ones to the data. This column represent x feature for theta_0.\n",
    "- Use learninig rate = 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = np.ones(len(X1))\n",
    "X_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generalize your implementation to work for MultiVariables\n",
    "#### Vectorize your implementation.\n",
    "<b>Notes:</b> \n",
    "- You need to add column that contains ones to the data. This column represent x feature for theta_0.\n",
    "- Use learninig rate = 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dimentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta_matrix = np.zeros(4).reshape(4,1) # THETA_0.....THETA_3\n",
    "cost = []\n",
    "lr = 0.0001\n",
    "m = len(X_matrix)\n",
    "y2 = y2.reshape(len(y2),1) # FROM (25,) TO (25,1)\n",
    "x_theta0 = np.ones(len(X_matrix)).reshape(len(X_matrix),1)#the column which multiplied with theta_0\n",
    "X_matrix_2=np.concatenate((x_theta0,X_matrix),axis=1) # (25,4)\n",
    "for i in range(2000):\n",
    "    h_x_mv=np.matmul(X_matrix_2,theta_matrix) #(25,4) *(4.1) >>(25,1)\n",
    "    err=h_x_mv-y2\n",
    "   # print(np.shape(err),np.shape(X2_new))\n",
    "    MSE_mv=(err.T@err)/(2*m)\n",
    "    d_theta_matrix=(X_matrix_2.T@err)/m\n",
    "    theta_matrix=theta_matrix-(lr*d_theta_matrix)\n",
    "    print(\"Iteration\", i)\n",
    "    print(\"**************\")\n",
    "   # print(theta_matrix)\n",
    "   \n",
    "    cost=np.append(cost,MSE_mv)\n",
    "    print(\"cost function:\", MSE_mv)\n",
    "    gradient_vector_norm = LA.norm(d_theta_matrix,1)\n",
    "    print(\"gradient_vector_norm\",gradient_vector_norm)\n",
    "    if gradient_vector_norm <=.001 :\n",
    "        print(\"best j == \",MSE_mv )\n",
    "        break\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize your implementation to work for MultiVariables\n",
    "#### Vectorize your implementation.\n",
    "<b>Notes:</b> \n",
    "- You need to add column that contains ones to the data. This column represent x feature for theta_0.\n",
    "- Use learninig rate = 0.0001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict y values using the LR equation\n",
    "##### h(x)= theta_0 + theta_1 * x1 + theta_2 * x2 + theta_3 * x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thata_0 , thata_1 , theta_2, theta_3 ,lr_2 = 0,0,0,0,0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss vs. iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.arange(len(cost)), cost, marker='*')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_x_mv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use R2 score to evaluate LR equation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y2,h_x_mv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
